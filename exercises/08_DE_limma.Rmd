---
title: "Differential Expression Analysis with limma"
author: "Carson Stacy & Jeffrey Lewis"
date: "Fall 2023"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

last updated: `r Sys.Date()`

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed("1492")
```

# Getting Things Setup

As usual, make sure we have the right packages for this exercise

```{r ready-packages}
if (!require("pacman")) install.packages("pacman"); library(pacman)

# let's load all of the files we were using and want to have again today
p_load("tidyverse", "knitr", "readr",
       "pander", "BiocManager", 
       "dplyr", "stringr", 
       "statmod", # required dependency, need to load manually on some macOS versions.
       "Glimma", # beautifies limma results
       "purrr", # for working with lists (beautify column names)
       "reactable") # for pretty tables.

# We also need these Bioconductor packages today.
p_load("edgeR", "AnnotationDbi", "org.Sc.sgd.db")
#NOTE: edgeR loads limma as a dependency
```

# Description

This will be our last differential expression analysis workflow,
converting gene counts across samples into meaningful information about
genes that appear to be significantly differentially expressed between
samples

# Learning outcomes

At the end of this exercise, you should be able to:

-   Generate a table of sample metadata.
-   Filter low counts and normalize count data.
-   Utilize the limma package to identify differentially expressed
    genes.

```{r load-libraries}
library(limma)
library(org.Sc.sgd.db)
# for ease of use, set max number of digits after decimal
options(digits=3)
```

# Loading in the count data file

We are downloading the counts for the non-subsampled fastq files from a Github repository using the code below. Just as in previous exercises, assign the data to the variable `counts`. You can change the file path if you have saved it to your computer in a different location.

```{r load-fc, error=TRUE}
counts <- read_tsv('https://github.com/clstacy/GenomicDataAnalysis_Fa23/raw/main/data/ethanol_stress/counts/salmon.gene_counts.merged.nonsubsamp.tsv',
                   col_names = TRUE) %>%
  # when we saved the tsv file, it converted the rownames to a column,
  # we are converting it back with this piped command.
  column_to_rownames("Name")
```

If you don't have that file for any reason, the below code chunk will
load a copy of it from Github.

To find the order of files we need, we can get just the part of the
column name before the first "." symbol with this command:

```{r identify-sampleOrder}
str_split_fixed(counts %>% colnames(), "\\.", n = 2)[, 1]
```

```{r generate-metadata}
sample_metadata <- tribble(
  ~Sample,                      ~Genotype,    ~Condition,
  "YPS606_MSN24_ETOH_REP1_R1",   "msn24dd",   "EtOH",
  "YPS606_MSN24_ETOH_REP2_R1",   "msn24dd",   "EtOH",
  "YPS606_MSN24_ETOH_REP3_R1",   "msn24dd",   "EtOH",
  "YPS606_MSN24_ETOH_REP4_R1",   "msn24dd",   "EtOH",
  "YPS606_MSN24_MOCK_REP1_R1",   "msn24dd",   "unstressed",
  "YPS606_MSN24_MOCK_REP2_R1",   "msn24dd",   "unstressed",
  "YPS606_MSN24_MOCK_REP3_R1",   "msn24dd",   "unstressed",
  "YPS606_MSN24_MOCK_REP4_R1",   "msn24dd",   "unstressed",
  "YPS606_WT_ETOH_REP1_R1",      "WT",        "EtOH",
  "YPS606_WT_ETOH_REP2_R1",      "WT",        "EtOH",
  "YPS606_WT_ETOH_REP3_R1",      "WT",        "EtOH",
  "YPS606_WT_ETOH_REP4_R1",      "WT",        "EtOH",
  "YPS606_WT_MOCK_REP1_R1",      "WT",        "unstressed",
  "YPS606_WT_MOCK_REP2_R1",      "WT",        "unstressed",
  "YPS606_WT_MOCK_REP3_R1",      "WT",        "unstressed",
  "YPS606_WT_MOCK_REP4_R1",      "WT",        "unstressed") %>%
  # Create a new column that combines the Genotype and Condition value
  mutate(Group = factor(
    paste(Genotype, Condition, sep = "."),
    levels = c(
      "WT.unstressed","WT.EtOH",
      "msn24dd.unstressed", "msn24dd.EtOH"
    )
  )) %>%
  # make Condition and Genotype a factor (with baseline as first level) for edgeR
  mutate(
    Genotype = factor(Genotype,
                      levels = c("WT", "msn24dd")),
    Condition = factor(Condition,
                       levels = c("unstressed", "EtOH"))
  )
  

```

Now, let's create a design matrix with this information

```{r}
group <- sample_metadata$Group
design <- model.matrix(~ 0 + group)

# beautify column names
colnames(design) <- levels(group)
design
```

# Count loading and Annotation

The count matrix is used to construct a DGEList class object. This is
the main data class in the edgeR package. The DGEList object is used to
store all the information required to fit a generalized linear model to
the data, including library sizes and dispersion estimates as well as
counts for each gene.

```{r load-counts}
y <- DGEList(counts, group=group)
colnames(y) <- sample_metadata$Sample
y$samples
```

Human-readable gene symbols can also be added to complement the gene ID
for each gene, using the annotation in the org.Sc.sgd.db package.

```{r}
y$genes <- AnnotationDbi::select(org.Sc.sgd.db,keys=rownames(y),columns="GENENAME")

head(y$genes)
```

# Filtering to remove low counts

Genes with very low counts across all libraries provide little evidence
for differential ex- pression. In addition, the pronounced discreteness
of these counts interferes with some of the statistical approximations
that are used later in the pipeline. These genes should be filtered out
prior to further analysis. Here, we will retain a gene only if it is
expressed at a count-per-million (CPM) above 60 in at least four
samples.

```{r filter-lowCounts}
keep <- rowSums(cpm(y) > 0.7) >= 4
y <- y[keep,]
summary(keep)
```

Where did those cutoff numbers come from?

As a general rule, we don't want to exclude a gene that is expressed in
only one group, so a cutoff number equal to the number of replicates can
be a good starting point. For counts, a good threshold can be chosen by
identifying the CPM that corresponds to a count of 10, which in this
case would be about 60 (due to our fastq files being subsets of the full
reads):

```{r estimate-cpmCutoff}
cpm(10, mean(y$samples$lib.size))
```

Smaller CPM thresholds are usually appropriate for larger libraries.

# Normalization for composition bias

TMM normalization is performed to eliminate composition biases between
libraries. This generates a set of normalization factors, where the
product of these factors and the library sizes defines the effective
library size. The calcNormFactors function returns the DGEList argument
with only the norm.factors changed.

```{r calc-normFactors}
y <- calcNormFactors(y)
y$samples
```

The normalization factors multiply to unity across all libraries. A
normalization factor below unity indicates that the library size will be
scaled down, as there is more suppression (i.e., composition bias) in
that library relative to the other libraries. This is also equivalent to
scaling the counts upwards in that sample. Conversely, a factor above
unity scales up the library size and is equivalent to downscaling the
counts. The performance of the TMM normalization procedure can be
examined using mean- difference (MD) plots. This visualizes the library
size-adjusted log-fold change between two libraries (the difference)
against the average log-expression across those libraries (the mean).
The below command plots an MD plot, comparing sample 1 against an
artificial library constructed from the average of all other samples.

```{r plotMDS, fig.show="hold", out.width="25%"}
for (sample in 1:nrow(y$samples)) {
  plotMD(cpm(y, log=TRUE), column=sample)
  abline(h=0, col="red", lty=2, lwd=2)
}
```

# Exploring differences between libraries

The data can be explored by generating multi-dimensional scaling (MDS)
plots. This visualizes the differences between the expression profiles
of different samples in two dimensions. The next plot shows the MDS plot
for the yeast heatshock data.

```{r plot-MDS}
points <- c(1,1,2,2)
colors <- rep(c("black", "red"),8)
plotMDS(y, col=colors[group], pch=points[group])
# legend("bottomright", legend=levels(group),
     # pch=points, col=colors, ncol=2)
legend("bottomright",legend=levels(group),
       pch=points, col=colors, ncol=2,
       inset=c(0,1.05), xpd=TRUE)

```

# Estimate Dispersion

This is the first step in a limma analysis that differs from the edgeR
workflow.

```{r estimate-dispersion}
y <- voom(y, design, plot = T,)

# compare this to the edgeR function estimateDisp, which uses a NB distribution.
# y <- estimateDisp(y, design, robust=TRUE)
# plotBCV(y)
```

What is voom doing?

-   Counts are transformed to log2 counts per million reads (CPM), where
    "per million reads" is defined based on the normalization factors we
    calculated earlier

-   A linear model is fitted to the log2 CPM for each gene, and the
    residuals are calculated

-   A smoothed curve is fitted to the sqrt(residual standard deviation)
    by average expression (see red line in plot above)

-   The smoothed curve is used to obtain weights for each gene and
    sample that are passed into limma along with the log2 CPMs.

Limma uses the `lmFit` function. This returns a MArrayLM object
containing the weighted least squares estimates for each gene.

```{r generate-fit}
fit <- lmFit(y, design)
head(coef(fit))

# edgeR equivalent
# fit <- glmQLFit(y, design, robust=TRUE)
# head(fit$coefficients)
# plotQLDisp(fit)
```

Comparisons between groups (log fold-changes) are obtained as
*contrasts* of these fitted linear models:

# Testing for differential expression

The final step is to actually test for significant differential
expression in each gene, using the QL F-test. The contrast of interest
can be specified using the `makeContrasts` function in limma, the same
one that is used by edgeR.

```{r fit-contrasts}
# generate contrasts we are interested in learning about
my.contrasts <- makeContrasts(EtOHvsMOCK.WT = WT.EtOH - WT.unstressed, 
                     EtOHvsMOCK.MSN24dd = msn24dd.EtOH - msn24dd.unstressed,
                     EtOH.MSN24ddvsWT = msn24dd.EtOH - WT.EtOH,
                     MOCK.MSN24ddvsWT = msn24dd.unstressed - WT.unstressed,
                     EtOHvsWT.MSN24ddvsWT = (msn24dd.EtOH-msn24dd.unstressed)-(WT.EtOH-WT.unstressed),
                     levels=design)

# fit the linear model to these contrasts
res_all <- contrasts.fit(fit, my.contrasts)

# This looks at all of our contrasts in my.contrasts
res_all <- eBayes(res_all)

# eBayes is the alternative to glmQLFTest in edgeR
# This contrast looks at the difference in the stress responses between mutant and WT
# res <- glmQLFTest(fit, contrast = my.contrasts)
```

```{r create-TableAll}

top.table <- topTable(res_all, sort.by = "F", n = Inf)
head(top.table, 20)

top.table %>% 
  tibble() %>% 
  arrange(adj.P.Val) %>%
  mutate(across(where(is.numeric), signif, 3)) %>%
  reactable()

# edgeR equivalent below:

# let's take a quick look at the results
# topTags(res, n=10) 
# 
# # generate a beautiful table for the pdf/html file.
# topTags(res, n=Inf) %>% data.frame() %>% 
#   arrange(FDR) %>%
#   mutate(logFC=round(logFC,2)) %>%
#   mutate(across(where(is.numeric), signif, 3)) %>%
#   reactable()
```

```{r summarize-DEgenes, fig.height=7, fig.width=10}
# Let's see how many genes in total are significantly different in any contrast
length(which(top.table$adj.P.Val < 0.05))

# let's summarize this and break it down by contrast.
res_all %>%
  decideTests(p.value = 0.05, lfc = 0) %>%
  summary()

# Bonus: limma allows us to create a venn diagram of these contrasts 
# up & downregulated genes
res_all %>%
  decideTests(p.value = 0.05, lfc = 1) %>% 
  vennDiagram(include=c("up", "down"),
              lwd=0.75,
              mar=rep(2,4), # increase margine size
              counts.col= c("red", "blue"),
              show.include=TRUE)

```

## Looking at a specific contrast

It is interesting to see all of the contrasts simultaneously, but often
we may want to look at just a single contrast (and get the corresponding
probabilities). Here is how we do that:

```{r fit-contrast}
# fit the linear model to these contrasts
res <- contrasts.fit(fit, my.contrasts[,"EtOHvsWT.MSN24ddvsWT"])

# This contrast looks at the difference in the stress responses between mutant and WT
res <- eBayes(res)
```

```{r create-Table}
# Note that there is no longer an "F" column, because we only look at one contrast.
top.table <- topTable(res, sort.by = "P", n = Inf)
head(top.table, 20)

top.table %>% 
  tibble() %>% 
  arrange(adj.P.Val) %>%
  mutate(across(where(is.numeric), signif, 3)) %>%
  reactable()


is.de <- decideTests(res, p.value=0.05)
summary(is.de)

# visualize results
plotMA(res, status=is.de)
```

## See the DE genes

```{r visualize-contrast}
res
```


We need to make sure and save our output file(s).

```{r save-res}
# Choose topTags destination
dir_output_limma <-
  path.expand("~/Desktop/Genomic_Data_Analysis/Analysis/limma/")
if (!dir.exists(dir_output_limma)) {
  dir.create(dir_output_limma, recursive = TRUE)
}

# for shairng with others, the topTags output is convenient.
top.table %>% tibble() %>%
  arrange(desc(adj.P.Val)) %>%
  mutate(adj.P.Val = round(adj.P.Val, 2)) %>%
  mutate(across(where(is.numeric), signif, 3)) %>%
  write_tsv(., file = paste0(dir_output_limma, "yeast_topTags_limma.tsv"))

# for subsequent analysis, let's save the res object as an R data object.
saveRDS(object = res, file = paste0(dir_output_limma, "yeast_res_limma.Rds"))

# we might also want our y object list
saveRDS(object = y, file = paste0(dir_output_limma, "yeast_y_limma.Rds"))
```


## Questions

Question 1: How many genes were upregulated and downregulated in the contrast we looked at in todays activity? Be sure to clarify the cutoffs used for determining significance.

Question 2: We analyzed the counts generated by Rsubread. Load in the counts generated by Salmon and look at the same contrast we did in class. What differences and similarities do you see?

A template for doing this is below:

```{r load-salmon}
path_salmon_counts <- path.expand("~/Desktop/Genomic_Data_Analysis/Data/Counts/Salmon/salmon.gene_counts.merged.yeast.tsv")
salmon_counts <- read_tsv(file = path_salmon_counts) %>%
  column_to_rownames("Name") #need to do this to get geneID as rownames
```

```{r repeat-edgeRWorkflow}
# We are reusing the sample_metadata, group, etc that we assigned above

# create DGEList with salmon counts
y_salmon <- DGEList(salmon_counts, group=group)
colnames(y_salmon) <- sample_metadata$Sample

# add gene names
y_salmon$genes <- AnnotationDbi::select(org.Sc.sgd.db,keys=rownames(y_salmon),
                                        columns="GENENAME")

# filter low counts
keep_salmon <- rowSums(cpm(y_salmon) > 60) >= 4
y_salmon <- y_salmon[keep_salmon,]

# calculate norm factors
y_salmon <-  calcNormFactors(y_salmon)

# estimate dispersion
y_salmon <- estimateDisp(y_salmon, design, robust=TRUE)

# generate the fit
fit_salmon <- glmQLFit(y_salmon, design, robust=TRUE)

# test our contrast of interest
res_salmon <- glmQLFTest(fit_salmon, contrast = my.contrasts[,"EtOHvsWT.MSN24ddvsWT"])

# generate a beautiful table for the pdf/html file.
topTags(res_salmon, n=Inf) %>% 
  data.frame() %>% 
  arrange(FDR) %>%
  mutate(logFC=round(logFC,2)) %>%
  mutate(across(where(is.numeric), signif, 3)) %>%
  reactable()

# summarize the DE genes
is.de_salmon <- decideTestsDGE(res_salmon, p.value=0.05)
summary(is.de_salmon)

# visualize results
plotSmear(res_salmon, de.tags=rownames(res_salmon)[is.de_salmon!=0])
```

Be sure to knit this file into a pdf or html file once you're finished.

System information for reproducibility:

```{r}
pander::pander(sessionInfo())
```
